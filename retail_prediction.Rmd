---
title: "Retail_sales_prediction"
author: "Chung-Hao Lee"
date: "8/22/2021"
output: github_document
---

```{r}
### Setting up environment
library("tidyverse") # for ggplot2
library("timetk") # for time series visualization
library("modeltime") # for arima model building
library("cowplot") # for combining multiple plots
library("parsnip") # for arima model building
```

```{r}
knitr::opts_chunk$set(
  collapse = TRUE, 
  comment = "#>",
  fig.path = "fig/figures/README-",
  out.width = "100%",
  message=FALSE, 
  warning=FALSE
)
```

```{r}
### Loading dataset

####Total Business Sales, Not Seasonally Adjusted
df_sales <- 
  read_csv("/Users/yginger/Desktop/Data Analytics/Time Series/Retail sales prediction/Inventory and sales dataset/TOTBUSSMNSA.csv") %>% 
  select(-c(realtime_start, realtime_end)) %>% 
  rename(sales = value) %>% 
  mutate(trend = row_number(),
         quarter = as.factor(lubridate::quarter(date)),
         month = as.factor(lubridate::month(date)))

df_inv <- 
  read_csv("/Users/yginger/Desktop/Data Analytics/Time Series/Retail sales prediction/Inventory and sales dataset/TOTBUSIMNSA.csv") %>% 
  select(-c(realtime_start, realtime_end))%>% 
  rename(inv = value) %>% 
  mutate(trend = row_number(),
         quarter = as.factor(lubridate::quarter(date)),
         month = as.factor(lubridate::month(date)))

str(df_sales)
str(df_inv)
```


# EDA
```{r}
### Plot sales and inv
df_sales %>% 
  plot_time_series(date, sales, .smooth=FALSE, .interactive = FALSE,.title = "Sales")

df_inv %>% 
  plot_time_series(date, inv, .smooth=FALSE, .interactive = FALSE, .title = "Inventory")

```

* We can see that both sales and inventory have similar patterns and have hung drops in 2008, due to financial crisis.

```{r}
###Seasonal plot for US retail sales from 2000
df_sales %>% 
  filter(date >= '2000-01-01') %>% 
  group_by(lubridate::year(date)) %>% 
  mutate(sales = scale(sales) - scale(sales)[1]) %>% 
  ungroup() %>% 
  plot_time_series(month, sales, 
                   .smooth= FALSE, 
                   .color_var = lubridate::year(date), 
                   .interactive = FALSE, 
                   .color_lab = "Year", 
                   .title='Seasonal plot for US retail sales from 2000',
                   .y_lab='Sales', 
                   .x_lab='Month')
```

* Excpt 2008, every years sales have upward trends in a year since 2000.

```{r}
# Create a seasonal subseries plot for the subset data starting from 2000

df_sales %>%
  filter(date >= '2000-01-01') %>%
  plot_time_series(
        .date = date,
        .value = sales,
        .facet_vars = month,
        .facet_ncol = 12, 
        .facet_scales = "fixed",
        .interactive = FALSE,
        .legend_show = FALSE,
        .title = "Seasonal subseries plot for US retail sales from 2000",
        .x_lab = "Year",
        .y_lab = "Sales ($ million)")  + 
        theme(axis.text.x=element_text(angle=60, hjust=1)
    )
```

* We can see that in every months, sales increases since 2000.

```{r}
# Create a STL decomposition plot for the full data
df_sales %>%
  plot_stl_diagnostics(
    date, sales,
    # Set features to return, desired frequency and trend
    .feature_set = c("observed", "season", "trend", "remainder"), 
    .interactive = FALSE)
```

* We use seasonal decomposition of time series by Loess (STL) to decompose the time series to season, trend and reminder. We can see in STL plot, sales has clear seasonal pattern and upward trend.


* In order to predict, let's split the data first. We will explore training data and build up models then we will use our models to predict testing data.


# ARIMA Model building
```{r}
# Split the data
# We intended include 2008, because in 2008, there is rare financial crisis occurs, we want to train ARIMA model with rare event. Leave 5 years for testing. 
df_sales_train <- df_sales %>% filter(date < '2015-01-01')
df_sales_test<- df_sales %>% filter(date >= '2015-01-01')
```

```{r}
# Unit root test: ADF test

df_sales_train %>%
  select(sales) %>% 
  ts(start = c(1992, 1), end = c(2014, 12), frequency = 12) %>% 
  tseries::adf.test()
```

* From ADF test, p-value > 0.05. We can say it's not stationary. We will further exam its stationary status by ACF and PACF. 

```{r}
### ACF and PACF
df_sales_train%>%
  plot_acf_diagnostics(date, sales, 
                       .lags = 60,
                       .show_white_noise_bars =TRUE,
                       .interactive = FALSE)
```

* ACF plot shows that sales are self-related and has seasonal pattern every 12 lags. PACF plot also show there is a spike exceed threshold at lag = 12. From this perspective, we can do differencing to make time series become stationary.

```{r}
# ACF and PACT after 1st order ordinary differencing

df_sales_train%>%
  plot_acf_diagnostics(date, diff_vec(df_sales_train$sales, difference = 1), 
                       .lags = 60,
                       .show_white_noise_bars =TRUE,
                       .interactive = FALSE)
```

* In ACF plot, even 12, 24, 36, 48 have peaks over threshold, this may cause by seasonal effect. It is closer to stationary compared to the plot before first order differencing. In PACF plot, lag = 12 also exist a peak over threshold, this may match the result we observe from ACF plot that this may cause by seasonal effect. So we will do seasonal differencing afterward.

```{r}
# After 1st order ordinary differencing and 1st order seasonal differencing

df_sales_train%>%
  mutate(difford1 = diff_vec(sales, difference = 1), difford1seas1 = diff_vec(difford1, lag = 12, difference = 1)) %>% 
  plot_acf_diagnostics(date, difford1seas1, 
                       .lags = 60,
                       .show_white_noise_bars =TRUE,
                       .interactive = FALSE)
```
```{r}
# After 1st order ordinary differencing and 2nd order seasonal differencing

df_sales_train%>%
  mutate(difford1 = diff_vec(sales, difference = 1), difford1seas2 = diff_vec(difford1, lag = 12, difference = 2)) %>% 
  plot_acf_diagnostics(date, difford1seas2, 
                       .lags = 60,
                       .show_white_noise_bars =TRUE,
                       .interactive = FALSE)
```

* In 1st order ordinary differencing and 1st order seasonal differencing, we see that the effect is not quite good, so I add one more on seasonal differencing and do 1st order ordinary differencing and 2nd order seasonal differencing. This time, we have a better result and more lags are fall inside threshold.

```{r}
# Run ADF test again after 1st order ordinary differencing and 2nd order seasonal differencing

df_sales_train %>%
  mutate(difford1 = diff_vec(sales, difference = 1), difford1seas2 = diff_vec(difford1, lag = 12, difference = 2)) %>% 
  select(difford1seas2) %>% 
  drop_na() %>% 
  ts(start = c(1992, 1), end = c(2011, 12), frequency = 12) %>% 
  tseries::adf.test()
```

* We can see that after 1st order ordinary differencing and 2nd order seasonal differencing, ADF test shows that p-value is way smaller than 0.01 compared to p-value of original time series 0.04. We can say after 1st order ordinary differencing and 2nd order seasonal differencing, the time series becomes stationary. 


* Next we will start to build ARIMA model. ARIMA is abbreviation of AutoRegression (AR) + Integrated (I) + Moving Average (MA). It has 3 parameters p, d, q in ARIMA model. ‘p’ is the order of the ‘Auto Regressive’ (AR) term. It refers to the number of lags of Y to be used as predictors. And ‘q’ is the order of the ‘Moving Average’ (MA) term. It refers to the number of lagged forecast errors that should go into the ARIMA Model. The value of d is the minimum number of differencing needed to make the series stationary. And if the time series is already stationary, then d = 0.

* Because right now we only deal with ARIMA not yet including seasonal effect, we will only look at ACF/PACF plots after 1st order differencing. From the previous differencing, we know that after 1st order differencing, the time series becomes stationary. So our d = 1. Next, we are going to find p and q.

* To find out p, we can look at PACF, because it's clearly that within 2 lags the AR is significant (out of threshold), This means we can use p = 2.

* Just like how we looked at the PACF plot for the number of AR terms (p), we can look at the ACF plot for the number of MA terms (q). From ACF plot, we can see that within 1 lags the MA is significant. This means we can use q = 1.

* So far we find out all p, q, d at p = 2, q = 1, d = 1. Then we can start to build ARIMA model of these 3 parameters.

```{r}
# Build an ARIMA model based on p = 2, q = 1, d = 1
arima_manu <- 
  arima_reg(mode = "regression",
  seasonal_period = 12,
  non_seasonal_ar = 2,
  non_seasonal_differences = 1,
  non_seasonal_ma = 1) %>% 
  set_engine("arima") %>%
  generics::fit(sales ~ date, data = df_sales_train)

arima_manu
```

```{r}
# Plot the residuals for ARIMA (2,1,1) model
arima_residuals <- arima_manu %>% 
  modeltime_table() %>% 
  modeltime_calibrate(new_data = df_sales_train) %>%
  modeltime_residuals() %>%
  plot_modeltime_residuals(.type = ("timeplot"),
                           .legend_show = FALSE, 
                           .interactive = FALSE)

# Plot the residuals distribution density for ARIMA (2,1,1) model
arima_density <- arima_manu %>% 
  modeltime_table() %>% 
  modeltime_calibrate(new_data = df_sales_train) %>%
  modeltime_residuals() %>% 
  ggplot()+
  geom_density(aes(x=.residuals))+
  labs(title = "Density Plot", x = "residuals")

plot_grid(arima_residuals, arima_density, nrow = 2, align = "v")
```

* As we can see from residuals plots that there is only slight patterns in residuals plot. From density plot, mean is near but not at zero. This may cause by seasonal effect, because at this stage, we haven't include seasonal ARIMA yet.

* Next we will start to build seasonal ARIMA

* Same rule applying to find P, D, Q in seasonal ARIMA. From previous differencing we know that it requires 2nd order seasonal differencing to make most of ACF within threshold. Therefore, we know D = 2. 

* With the similar procedure finding p and q in ARIMA, we find P = 1 and Q = 1 in seasonal ARIMA.

```{r}
# Build a seasonal ARIMA model based on P = 1, D = 2, Q = 1
sarima_manu <- 
  arima_reg(mode = "regression",
  seasonal_period = 12,
  non_seasonal_ar = 2,
  non_seasonal_differences = 1,
  non_seasonal_ma = 1,
  seasonal_ar = 1,
  seasonal_differences = 2,
  seasonal_ma = 1) %>% 
  set_engine("arima") %>%
  generics::fit(sales ~ date, data = df_sales_train)

sarima_manu
```

* After building seasonal ARIMA(2,1,1)(1,2,1), we can see AIC of seasonal ARIMA is 5876 is smaller than AIC of ARIMA 6745, which is great. This means our model improves.

```{r}
# Plot the residuals for sARIMA (2,1,1)(1,2,1) model
sarima_residuals <- sarima_manu %>% 
  modeltime_table() %>% 
  modeltime_calibrate(new_data = df_sales_train) %>%
  modeltime_residuals() %>%
  plot_modeltime_residuals(.type = ("timeplot"),
                           .legend_show = FALSE, 
                           .interactive = FALSE)

# Plot the residuals distribution density for sARIMA (2,1,1)(1,2,1) model
sarima_density <- sarima_manu %>% 
  modeltime_table() %>% 
  modeltime_calibrate(new_data = df_sales_train) %>%
  modeltime_residuals() %>% 
  ggplot()+
  geom_density(aes(x=.residuals))+
  labs(title = "Density Plot", x = "residuals")

plot_grid(sarima_residuals, sarima_density, nrow = 2, align = "v")
```

* Voila! Compared to ARIMA model, seasonal ARIMA model has no pattern in residual plot and has mean at 0 and uniform variance. This means our seasonal ARIMA model has explained almost all time series data.

* In fact, there is a function called auto arima, which can automatically build ARIMA (including seasonal, if needed). So next, we will use this function to build an auto ARIMA.

```{r}
# Build an auto ARIMA model

arima_auto <- 
  arima_reg() %>% 
  set_engine("auto_arima") %>%
  fit(sales ~ date, data = df_sales_train)

arima_auto
```

* Auto ARIMA picks p = 1, d = 0, q = 3, P = 2, D = 1, Q = 2. This is different from our own seasonal ARIMA model. The AIC of auto ARIMA is 6002, which is slightly larger than our seasonal ARIMA's AIC 5876. This means our seasonal ARIMA is better than auto ARIMA. But because the difference is so small, we will keep both models to do further prediction.

```{r}
# Plot the residuals for  auto ARIMA (1,0,3)(2,1,2) model
arima_auto_residuals <- arima_auto %>% 
  modeltime_table() %>% 
  modeltime_calibrate(new_data = df_sales_train) %>%
  modeltime_residuals() %>%
  plot_modeltime_residuals(.type = ("timeplot"),
                           .legend_show = FALSE, 
                           .interactive = FALSE)

# Plot the residuals distribution density for sARIMA (2,1,1)(1,2,1) model
arima_auto_density <- arima_auto %>% 
  modeltime_table() %>% 
  modeltime_calibrate(new_data = df_sales_train) %>%
  modeltime_residuals() %>% 
  ggplot()+
  geom_density(aes(x=.residuals))+
  labs(title = "Density Plot", x = "residuals")

plot_grid(arima_auto_residuals, arima_auto_density, nrow = 2, align = "v")
```

* The residuals and density plots of auto ARIMA are good but not good enough. In density plot of auto ARIMA, mean is near 0 but not at 0. This also match finding that AIC of auto ARIMA is larger than our seasonal ARIMA model.


# Prediction
```{r}
# Model table and calibration

models_tbl <- modeltime_table(sarima_manu,
                              arima_auto)

calibration_tbl <- models_tbl %>%
    modeltime_calibrate(new_data = df_sales_test)

calibration_tbl %>%
  modeltime_accuracy()%>%
  table_modeltime_accuracy(.sortable = FALSE,
                           .searchable = FALSE,
                           .filterable = FALSE,
                           .expand_groups = FALSE,
                           .interactive = FALSE)
```

```{r}
### Check and plot forecast with actual data
calibration_tbl %>%
    modeltime_forecast(
        new_data    = df_sales_test,
        actual_data = df_sales
    ) %>%
    plot_modeltime_forecast(
      .legend_max_width = 25,
      .interactive      = FALSE
    )
```

* Well, apparently, both models perform well and almost follow the trend of actual data. Our seasonal ARIMA has 0.73 r squared, which means it can explain 73% of data. Auto ARIMA has 0.93 r squared.

Next, I'd like to add another variable to see if they can improve prediction model.


```{r}
# importing another dataset
df_econ <- 
  read_csv("/Users/yginger/Desktop/Data Analytics/Time Series/Retail sales prediction/Inventory and sales dataset/us-econ.csv") %>% 
  mutate(date = lubridate::mdy(date),
  trend = row_number()) %>% 
  filter(date >= '1992-01-01', date <= '2019-09-01') 

skimr::skim(df_econ)

# merge all tables
df_merge <- 
  df_sales %>% 
  bind_cols(df_inv)

df_merge <- 
  inner_join(df_merge, df_econ, by = c("trend...3" = "trend")) %>% 
  select(sales, date...2, trend...3, quarter...4, month...5, inv, income, saving, unemployment, CPI, inflation, population, HPI) %>% 
  rename(date = date...2, trend = trend...3, quarter = quarter...4, month = month...5)
```

```{r}
# Split the data
# We intended include 2008, because in 2008, there is rare financial crisis occurs, we want to train ARIMA model with rare event. Leave 5 years for testing. 
df_merge_train <- df_merge %>% filter(date < '2015-01-01')
df_merge_test<- df_merge %>% filter(date >= '2015-01-01')
```

```{r}
# Build up a linear model for prediction
linear_model <-
  linear_reg() %>%
  set_engine("lm") %>%
  fit(sales ~ date + CPI + income + saving + HPI + population + inflation + inv, data = df_merge_train)

summary(linear_model$fit)
```

* From the linear regression, we can see that income, inflation, inv all have p-value lower than 0.05, which means they are statistic significant. Adjusted R-squared is 0.96, which means the linear model can explain 96% of data.

```{r}
# Build a seasonal ARIMA model with another variables

# All variable
sarima_manu_var <- 
  arima_reg(mode = "regression",
  seasonal_period = 12,
  non_seasonal_ar = 2,
  non_seasonal_differences = 1,
  non_seasonal_ma = 1,
  seasonal_ar = 1,
  seasonal_differences = 2,
  seasonal_ma = 1) %>% 
  set_engine("arima") %>%
  fit(sales ~ date + CPI + income + saving + HPI + population + inflation + inv, data = df_merge_train)

# Variable that p-vaule < 0.05 in linear regression 
sarima_manu_var_imp <- 
  arima_reg(mode = "regression",
  seasonal_period = 12,
  non_seasonal_ar = 2,
  non_seasonal_differences = 1,
  non_seasonal_ma = 1,
  seasonal_ar = 1,
  seasonal_differences = 2,
  seasonal_ma = 1) %>% 
  set_engine("arima") %>%
  fit(sales ~ date + income + inflation + inv , data = df_merge_train)

# Variable that p-vaule < 0.05  and has the largest coefficient in linear regression
sarima_manu_var_imp_2 <- 
  arima_reg(mode = "regression",
  seasonal_period = 12,
  non_seasonal_ar = 2,
  non_seasonal_differences = 1,
  non_seasonal_ma = 1,
  seasonal_ar = 1,
  seasonal_differences = 2,
  seasonal_ma = 1) %>% 
  set_engine("arima") %>%
  fit(sales ~ date + income, data = df_merge_train)

sarima_manu
sarima_manu_var
sarima_manu_var_imp
sarima_manu_var_imp_2
```

* We can see that all seasonal ARIMA model with adding variables have smaller AIC than original seasonal ARIMA model. That means adding variables can improve our model.

```{r}
# Plot the residuals for seasonal ARIMA models

sarima_models_tbl_var <- modeltime_table(
  sarima_manu,
  sarima_manu_var,
  sarima_manu_var_imp,
  sarima_manu_var_imp_2)

sarima_models_tbl_var %>%
    modeltime_calibrate(new_data = df_merge_train) %>%
    modeltime_residuals() %>%
    plot_modeltime_residuals(.interactive = FALSE)
```

* All models has similar patterns in residual plots.

```{r}
# Build an auto ARIMA model with another variables

# All variable
arima_auto_var <- 
  arima_reg() %>% 
  set_engine("auto_arima") %>%
  fit(sales ~ date + CPI + income + saving + HPI + population + inflation + inv, data = df_merge_train)

# Variable that p-vaule < 0.05 in linear regression 
arima_auto_var_imp <- 
  arima_reg() %>% 
  set_engine("auto_arima") %>%
  fit(sales ~ date + income + inflation + inv , data = df_merge_train)

# Variable that p-vaule < 0.05  and has the largest coefficient in linear regression
arima_auto_var_imp_2 <- 
  arima_reg() %>% 
  set_engine("auto_arima") %>%
  fit(sales ~ date + income, data = df_merge_train)

arima_auto
arima_auto_var
arima_auto_var_imp
arima_auto_var_imp_2
```

* Interesting in auto ARIMA models with adding variables. All models have different p, d, q and P, D, Q. Also if we only add income to models, it has slightly higher AIC than original auto ARIMA model. This means only income adding cannot improve the model.

```{r}
# Plot the residuals for auto ARIMA models

auto_arima_models_tbl_var <- modeltime_table(
  arima_auto,
  arima_auto_var,
  arima_auto_var_imp,
  arima_auto_var_imp_2
)

auto_arima_models_tbl_var %>%
    modeltime_calibrate(new_data = df_merge_train) %>%
    modeltime_residuals() %>%
    plot_modeltime_residuals(.interactive = FALSE)

```

* All models has similar patterns in residual plots.


# Prediction with variables
```{r}
# Model table and calibration

models_tbl_var_all <- modeltime_table(
  linear_model,
  sarima_manu,
  sarima_manu_var,
  sarima_manu_var_imp,
  sarima_manu_var_imp_2,
  arima_auto,
  arima_auto_var,
  arima_auto_var_imp,
  arima_auto_var_imp_2)

calibration_tbl_var_all <- models_tbl_var_all %>%
    modeltime_calibrate(new_data = df_merge_test)

calibration_tbl_var_all %>%
  modeltime_accuracy()%>%
  table_modeltime_accuracy(.interactive = FALSE)
```

* None-surprisingly, linear model perform the worst and have the highest MAE, RMSE and the lowest r squared. Overall, the best model is our seasonal ARIMA model with adding all variables. In the auto ARIMA group, the best model is the original auto ARIMA model. 

```{r}
### Check forcast with actual data
calibration_tbl_var_all %>%
    modeltime_forecast(
        new_data    = df_merge_test,
        actual_data = df_merge
    ) %>%
    plot_modeltime_forecast(
      .legend_max_width = 5,
      .interactive      = FALSE
    )
```

* The all model forecasting plots. 

```{r}
### Check and plot forecast with actual data


modeltime_table(linear_model,
  sarima_manu_var,
  arima_auto) %>%
  modeltime_calibrate(new_data = df_merge_test) %>%
    modeltime_forecast(
        new_data    = df_merge_test,
        actual_data = df_merge
    ) %>%
    plot_modeltime_forecast(
      .legend_max_width = 25,
      .interactive      = FALSE
    )
```

* If we only pick out the linear model and the best model in seasonal ARIMA and auto ARIMA, we can see the both the best models in seasonal ARIMA and auto ARIMA have great forecast. However, linear model does not perform well.